{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMaq6flNmc0fZNfJ67h611O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lvllvl/SLAM/blob/main/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Section"
      ],
      "metadata": {
        "id": "cxYI1YlP14sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run script: Connect to google drive"
      ],
      "metadata": {
        "id": "mYLQgF8OynKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your drive, to save parts of your model, e.g., checkpoints, etc.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-NCSsTntvOK",
        "outputId": "0e5d2e5d-7184-4171-d54f-8d7c76974833"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Script: Git clone repos\n",
        "\n",
        "- comma10k repo\n",
        "- project repo (SLAM)"
      ],
      "metadata": {
        "id": "aQPkyTqpy5GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clone repo, org folder structures"
      ],
      "metadata": {
        "id": "YhPK5DgMNSPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def clone_repo(repo_url, dest_path):\n",
        "    if os.path.exists(dest_path):\n",
        "        print(f\"Directory {dest_path} already exists. Removing and re-cloning.\")\n",
        "        shutil.rmtree(dest_path)\n",
        "    os.system(f\"git clone {repo_url} {dest_path}\")\n",
        "\n",
        "# Clone repos\n",
        "clone_repo('https://github.com/lvllvl/SLAM.git', 'SLAM')\n",
        "clone_repo('https://github.com/commaai/comma10k.git', 'comma10k')\n",
        "\n",
        "\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "def create_directory(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def move_files(src, dst):\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"Source directory {src} does not exist.\")\n",
        "        return\n",
        "    for filename in os.listdir(src):\n",
        "        src_file = os.path.join(src, filename)\n",
        "        dst_file = os.path.join(dst, filename)\n",
        "        try:\n",
        "            shutil.move(src_file, dst_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {src_file} to {dst_file}: {e}\")\n",
        "\n",
        "\n",
        "def move_selected_files(src, dst, prefix=None, suffix=None):\n",
        "    \"\"\"\n",
        "    Move files from src to dst based on prefix and/or suffix criteria.\n",
        "\n",
        "    :param src: Source directory\n",
        "    :param dst: Destination directory\n",
        "    :param prefix: Prefix to filter files (optional)\n",
        "    :param suffix: Suffix to filter files (optional)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"Source directory {src} does not exist.\")\n",
        "        return\n",
        "    for filename in os.listdir(src):\n",
        "        if prefix and not filename.startswith(prefix):\n",
        "            continue\n",
        "        if suffix and not filename.endswith(suffix):\n",
        "            continue\n",
        "        src_file = os.path.join(src, filename)\n",
        "        dst_file = os.path.join(dst, filename)\n",
        "        try:\n",
        "            shutil.move(src_file, dst_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {src_file} to {dst_file}: {e}\")\n",
        "\n",
        "\n",
        "# Create directories for training and validation\n",
        "create_directory('dataset_root/train/images')\n",
        "create_directory('dataset_root/train/masks')\n",
        "create_directory('dataset_root/val/images')\n",
        "create_directory('dataset_root/val/masks')\n",
        "\n",
        "# Move images and masks to training folder\n",
        "move_files('comma10k/imgs', 'dataset_root/train/images')\n",
        "move_files('comma10k/masks', 'dataset_root/train/masks')\n",
        "\n",
        "# Move images and masks to validation folder\n",
        "move_files('comma10k/imgs2', 'dataset_root/val/images')\n",
        "move_files('comma10k/masks2', 'dataset_root/val/masks')\n",
        "\n",
        "# Move SLAM *.py files to the dataset_root folder\n",
        "move_selected_files( 'SLAM/', 'dataset_root/', prefix=None, suffix='.py' )\n",
        "\n",
        "# List directories to verify\n",
        "print(os.listdir('.'))\n",
        "print(os.listdir('dataset_root'))\n",
        "\n",
        "# Update filenames\n",
        "import os\n",
        "\n",
        "def add_suffix_to_mask_filenames(image_dir, mask_dir, suffix):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    mask_files = os.listdir(mask_dir)\n",
        "\n",
        "    for image_file in image_files:\n",
        "        base, extension = os.path.splitext(image_file)\n",
        "        expected_mask_file = f\"{base}{suffix}{extension}\"\n",
        "        actual_mask_file = f\"{base}{extension}\"\n",
        "\n",
        "        if actual_mask_file in mask_files:\n",
        "            os.rename(os.path.join(mask_dir, actual_mask_file), os.path.join(mask_dir, expected_mask_file))\n",
        "            print(f\"Renamed {actual_mask_file} to {expected_mask_file}\")\n",
        "\n",
        "\n",
        "add_suffix_to_mask_filenames('/content/dataset_root/train/images', '/content/dataset_root/train/masks', '_mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A6wD4MwIrxa",
        "outputId": "e95ffa0c-f327-4734-d155-844550999cd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'dataset_root', 'SLAM', 'drive', 'comma10k', 'sample_data']\n",
            "['train.py', 'val', 'model.py', 'dataloader.py', 'validate.py', 'utils.py', 'config.py', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify all same number of files are in the folders\n",
        "\n",
        "!ls -l dataset_root/train/images | grep ^- | wc -l\n",
        "!ls -l dataset_root/train/masks | grep ^- | wc -l\n",
        "\n",
        "!ls -l dataset_root/val/images | grep ^- | wc -l\n",
        "!ls -l dataset_root/val/masks | grep ^- | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QBonUrdn4yW",
        "outputId": "cb8fc06b-30c2-4931-81f3-0259ee07d818"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9888\n",
            "9888\n",
            "2000\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Section"
      ],
      "metadata": {
        "id": "SWlps7qFFCSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentation Model: U-net Segmentaiton"
      ],
      "metadata": {
        "id": "fchpP5_WCGkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset_root/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7NAp5NYEt_2",
        "outputId": "ad1960d5-46e0-43fd-9525-c4dd615aa782"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary:\n",
            "UNet(\n",
            "  (enc_conv0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (enc_conv1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (enc_conv2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (enc_conv3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (bottleneck): Sequential(\n",
            "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (up_conv3): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec_conv3): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (up_conv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec_conv2): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (up_conv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec_conv1): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (up_conv0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (dec_conv0): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (final_conv): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "STAGE:2023-11-14 19:00:10 4758:4758 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "Batch 1 / 309, Current Loss: 1.5978\n",
            "Batch 2 / 309, Current Loss: 1.5827\n",
            "Batch 3 / 309, Current Loss: 1.5723\n",
            "Batch 4 / 309, Current Loss: 1.5615\n",
            "Batch 5 / 309, Current Loss: 1.5513\n",
            "Batch 6 / 309, Current Loss: 1.5421\n",
            "Batch 7 / 309, Current Loss: 1.5305\n",
            "Batch 8 / 309, Current Loss: 1.5231\n",
            "Batch 9 / 309, Current Loss: 1.5117\n",
            "Batch 10 / 309, Current Loss: 1.4986\n",
            "Batch 11 / 309, Current Loss: 1.4842\n",
            "Batch 12 / 309, Current Loss: 1.4716\n",
            "Batch 13 / 309, Current Loss: 1.4534\n",
            "Batch 14 / 309, Current Loss: 1.4306\n",
            "Batch 15 / 309, Current Loss: 1.4055\n",
            "Batch 16 / 309, Current Loss: 1.3836\n",
            "Batch 17 / 309, Current Loss: 1.3426\n",
            "Batch 18 / 309, Current Loss: 1.3184\n",
            "Batch 19 / 309, Current Loss: 1.2237\n",
            "Batch 20 / 309, Current Loss: 1.1497\n",
            "Batch 21 / 309, Current Loss: 0.9735\n",
            "Batch 22 / 309, Current Loss: 0.6341\n",
            "Batch 23 / 309, Current Loss: 0.1901\n",
            "Batch 24 / 309, Current Loss: 0.0179\n",
            "Batch 25 / 309, Current Loss: 0.0020\n",
            "Batch 26 / 309, Current Loss: 0.0001\n",
            "Batch 27 / 309, Current Loss: 0.0000\n",
            "Batch 28 / 309, Current Loss: 0.0000\n",
            "Batch 29 / 309, Current Loss: 0.0000\n",
            "Batch 30 / 309, Current Loss: 0.0000\n",
            "Batch 31 / 309, Current Loss: 0.0000\n",
            "Batch 32 / 309, Current Loss: 0.0000\n",
            "Batch 33 / 309, Current Loss: 0.0000\n",
            "Batch 34 / 309, Current Loss: 0.0000\n",
            "Batch 35 / 309, Current Loss: 0.0000\n",
            "Batch 36 / 309, Current Loss: 0.0000\n",
            "Batch 37 / 309, Current Loss: 0.0000\n",
            "Batch 38 / 309, Current Loss: 0.0000\n",
            "Batch 39 / 309, Current Loss: 0.0000\n",
            "Batch 40 / 309, Current Loss: 0.0000\n",
            "Batch 41 / 309, Current Loss: 0.0000\n",
            "Batch 42 / 309, Current Loss: 0.0000\n",
            "Batch 43 / 309, Current Loss: 0.0000\n",
            "Batch 44 / 309, Current Loss: 0.0000\n",
            "Batch 45 / 309, Current Loss: 0.0000\n",
            "Batch 46 / 309, Current Loss: 0.0000\n",
            "Batch 47 / 309, Current Loss: 0.0000\n",
            "Batch 48 / 309, Current Loss: 0.0000\n",
            "Batch 49 / 309, Current Loss: 0.0000\n",
            "Batch 50 / 309, Current Loss: 0.0000\n",
            "Batch 51 / 309, Current Loss: 0.0000\n",
            "Batch 52 / 309, Current Loss: 0.0000\n",
            "Batch 53 / 309, Current Loss: 0.0000\n",
            "Batch 54 / 309, Current Loss: 0.0000\n",
            "Batch 55 / 309, Current Loss: 0.0000\n",
            "Batch 56 / 309, Current Loss: 0.0000\n",
            "Batch 57 / 309, Current Loss: 0.0000\n",
            "Batch 58 / 309, Current Loss: 0.0000\n",
            "Batch 59 / 309, Current Loss: 0.0000\n",
            "Batch 60 / 309, Current Loss: 0.0000\n",
            "Batch 61 / 309, Current Loss: 0.0000\n",
            "Batch 62 / 309, Current Loss: 0.0000\n",
            "Batch 63 / 309, Current Loss: 0.0000\n",
            "Batch 64 / 309, Current Loss: 0.0000\n",
            "Batch 65 / 309, Current Loss: 0.0000\n",
            "Batch 66 / 309, Current Loss: 0.0000\n",
            "Batch 67 / 309, Current Loss: 0.0000\n",
            "Batch 68 / 309, Current Loss: 0.0000\n",
            "Batch 69 / 309, Current Loss: 0.0000\n",
            "Batch 70 / 309, Current Loss: 0.0000\n",
            "Batch 71 / 309, Current Loss: 0.0000\n",
            "Batch 72 / 309, Current Loss: 0.0000\n",
            "Batch 73 / 309, Current Loss: 0.0000\n",
            "Batch 74 / 309, Current Loss: 0.0000\n",
            "Batch 75 / 309, Current Loss: 0.0000\n",
            "Batch 76 / 309, Current Loss: 0.0000\n",
            "Batch 77 / 309, Current Loss: 0.0000\n",
            "Batch 78 / 309, Current Loss: 0.0000\n",
            "Batch 79 / 309, Current Loss: 0.0000\n",
            "Batch 80 / 309, Current Loss: 0.0000\n",
            "Batch 81 / 309, Current Loss: 0.0000\n",
            "Batch 82 / 309, Current Loss: 0.0000\n",
            "Batch 83 / 309, Current Loss: 0.0000\n",
            "Batch 84 / 309, Current Loss: 0.0000\n",
            "Batch 85 / 309, Current Loss: 0.0000\n",
            "Batch 86 / 309, Current Loss: 0.0000\n",
            "Batch 87 / 309, Current Loss: 0.0000\n",
            "Batch 88 / 309, Current Loss: 0.0000\n",
            "Batch 89 / 309, Current Loss: 0.0000\n",
            "Batch 90 / 309, Current Loss: 0.0000\n",
            "Batch 91 / 309, Current Loss: 0.0000\n",
            "Batch 92 / 309, Current Loss: 0.0000\n",
            "Batch 93 / 309, Current Loss: 0.0000\n",
            "Batch 94 / 309, Current Loss: 0.0000\n",
            "Batch 95 / 309, Current Loss: 0.0000\n",
            "Batch 96 / 309, Current Loss: 0.0000\n",
            "Batch 97 / 309, Current Loss: 0.0000\n",
            "Batch 98 / 309, Current Loss: 0.0000\n",
            "Batch 99 / 309, Current Loss: 0.0000\n",
            "Batch 100 / 309, Current Loss: 0.0000\n",
            "Batch 101 / 309, Current Loss: 0.0000\n",
            "Batch 102 / 309, Current Loss: 0.0000\n",
            "Batch 103 / 309, Current Loss: 0.0000\n",
            "Batch 104 / 309, Current Loss: 0.0000\n",
            "Batch 105 / 309, Current Loss: 0.0000\n",
            "Batch 106 / 309, Current Loss: 0.0000\n",
            "Batch 107 / 309, Current Loss: 0.0000\n",
            "Batch 108 / 309, Current Loss: 0.0000\n",
            "Batch 109 / 309, Current Loss: 0.0000\n",
            "Batch 110 / 309, Current Loss: 0.0000\n",
            "Batch 111 / 309, Current Loss: 0.0000\n",
            "Batch 112 / 309, Current Loss: 0.0000\n",
            "Batch 113 / 309, Current Loss: 0.0000\n",
            "Batch 114 / 309, Current Loss: 0.0000\n",
            "Batch 115 / 309, Current Loss: 0.0000\n",
            "Batch 116 / 309, Current Loss: 0.0000\n",
            "Batch 117 / 309, Current Loss: 0.0000\n",
            "Batch 118 / 309, Current Loss: 0.0000\n",
            "Batch 119 / 309, Current Loss: 0.0000\n",
            "Batch 120 / 309, Current Loss: 0.0000\n",
            "Batch 121 / 309, Current Loss: 0.0000\n",
            "Batch 122 / 309, Current Loss: 0.0000\n",
            "Batch 123 / 309, Current Loss: 0.0000\n",
            "Batch 124 / 309, Current Loss: 0.0000\n",
            "Batch 125 / 309, Current Loss: 0.0000\n",
            "Batch 126 / 309, Current Loss: 0.0000\n",
            "Batch 127 / 309, Current Loss: 0.0000\n",
            "Batch 128 / 309, Current Loss: 0.0000\n",
            "Batch 129 / 309, Current Loss: 0.0000\n",
            "Batch 130 / 309, Current Loss: 0.0000\n",
            "Batch 131 / 309, Current Loss: 0.0000\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDoXDT5DOdEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_fXsvivalOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}